{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc880131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pony in c:\\users\\max\\anaconda3\\lib\\site-packages (0.7.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pony"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c3a0b",
   "metadata": {},
   "source": [
    "## Library Import\n",
    "The below codes import the necessary libraries needed to work on the project. The libraries being imported are CSV, JSON, XML.etree.ElementTree, pymysql and io. This is necessary for reading, writing and processing CSV, JSON, XML and SQL files. Additionally, the Pony ORM library is imported for working with databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5032c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries Imported Successfully\n"
     ]
    }
   ],
   "source": [
    "#Import libraries needed \n",
    "import csv\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import io\n",
    "from pony.orm import *\n",
    "print('libraries Imported Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0cef4",
   "metadata": {},
   "source": [
    "## Storing Connection Details in Variables\n",
    "The below codes store the necessary information for connecting to the provided database into variables. This makes updating connection details easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081f68eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Stored\n"
     ]
    }
   ],
   "source": [
    "#input your sql database details here\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = ''\n",
    "database = 'bigdata'\n",
    "\n",
    "print('Successfully Stored')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cbc6b",
   "metadata": {},
   "source": [
    "## Global Variables\n",
    "The below codes are used to create global variables and lists to hold data for easy manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23ecb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Variables Created\n"
     ]
    }
   ],
   "source": [
    "#Creating Global Variables and providing a list to hold data for easy manipulation\n",
    "first_name = []\n",
    "last_name = []\n",
    "age = []\n",
    "sex = []\n",
    "veh_make = []\n",
    "veh_mod = []\n",
    "veh_year = []\n",
    "veh_type = []\n",
    "iban = []\n",
    "card_num = []\n",
    "card_sec_code = []\n",
    "card_sd =[]\n",
    "card_ed = []\n",
    "main_add = []\n",
    "city_add = []\n",
    "post_code = []\n",
    "retired = []\n",
    "dependants = []\n",
    "marital_status = []\n",
    "salary = []\n",
    "pension = []\n",
    "company = []\n",
    "comm_dist = []\n",
    "others = []\n",
    "headers_2 = ('firstName', 'lastName', 'age', 'iban', 'credit_card_number', 'credit_card_security_code', 'credit_card_start_date', 'credit_card_end_date', 'address_main', 'address_city', 'address_postcode', 'others')\n",
    "con_headers = ('cus_id', 'firstName', 'lastName', 'age', 'sex', 'retired', 'dependants', 'marital_status', 'salary', 'pension', 'company', 'comm_dist','veh_make', 'veh_mod', 'veh_year', 'veh_type', 'iban', 'card_num', 'card_sec_code', 'card_sd', 'card_ed', 'main_add', 'city_add', 'post_code')\n",
    "print(\"Global Variables Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511432e3",
   "metadata": {},
   "source": [
    "## Parsing files: CSV (Vehicle Information)\n",
    "The below codes read data from the provided CSV file (user_data.csv) and stores each columns in respective global variables while skipping the first line of the CSV file which contains the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c10f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Successfully Parsed\n"
     ]
    }
   ],
   "source": [
    "#open CSV file and append rows to global variable (Vehicle Information [logistics])\n",
    "with open(\"user_data.csv\", mode='r') as veh_det:\n",
    "    csv_reader = csv.reader(veh_det, delimiter=',')\n",
    "    for line_number, line in enumerate(csv_reader):\n",
    "        if line_number == 0: #skip the headers, not interested\n",
    "            continue\n",
    "        #storing the data from each row to corresponding Global Variable\n",
    "        first_name.append(line[0])\n",
    "        last_name.append(line[1])\n",
    "        age.append(line[2])\n",
    "        sex.append(line[3])\n",
    "        veh_make.append(line[4])\n",
    "        veh_mod.append(line[5])\n",
    "        veh_year.append(line[6])\n",
    "        veh_type.append(line[7])\n",
    "veh_det.close() #close file\n",
    "print(\"CSV File Successfully Parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4beb426",
   "metadata": {},
   "source": [
    "## Parsing files: JSON (Financial Information)\n",
    "The below code is used to parse the JSON file provided (user_data.json) and write the data into a CSV file. It then reads the CSV file and stores each row in the respective global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a44979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON File Successfully Parsed\n"
     ]
    }
   ],
   "source": [
    "#Financial Information\n",
    "\n",
    "#open JSON file, read it into a variable\n",
    "with open('user_data.json') as fin_det:\n",
    "    user_data = json.load(fin_det)\n",
    "\n",
    "#Create a new CSV file to write details from JSON file\n",
    "user_fin_det = open('user_fin_det.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(user_fin_det)\n",
    "\n",
    "# write new headers to align with global variable\n",
    "count = 0\n",
    "for item in user_data:\n",
    "    if count == 0:\n",
    "        #Writing headers of CSV file\n",
    "        header = headers_2\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    "#write rows\n",
    "for item in user_data:\n",
    "    csv_writer.writerow(item.values())\n",
    "#close csv file\n",
    "user_fin_det.close()\n",
    "\n",
    "#Merge with global variable\n",
    "with open(\"user_fin_det.csv\", mode='r') as fin_det:\n",
    "    csv_reader = csv.reader(fin_det, delimiter=',')\n",
    "    #user_data_csv = csv_reader\n",
    "    for line_number, line in enumerate(csv_reader):\n",
    "        if line_number == 0:\n",
    "            continue\n",
    "        first_name.append(line[0])\n",
    "        last_name.append(line[1])\n",
    "        age.append(line[2])\n",
    "        iban.append(line[3])\n",
    "        card_num.append(line[4])\n",
    "        card_sec_code.append(line[5])\n",
    "        card_sd.append(line[6])\n",
    "        card_ed.append(line[7])\n",
    "        main_add.append(line[8])\n",
    "        city_add.append(line[9])\n",
    "        post_code.append(line[10])\n",
    "fin_det.close()\n",
    "print(\"JSON File Successfully Parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3d10a",
   "metadata": {},
   "source": [
    "## Parsing files: XML (Employment Details)\n",
    "The below code is used to parse the XML file provided (user_data.xml) and write the data into a JSON file which is finally converted to a CSV file. It then reads the CSV file and stores each row in the respective global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf178af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML File Successfully Parsed\n"
     ]
    }
   ],
   "source": [
    "#***Parsing XML file\n",
    "tree = ET.parse('user_data.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "#list to hold data from JSON\n",
    "json_data = []\n",
    "\n",
    "#iterate over the root element and get each element\n",
    "for elem in root:\n",
    "    #dictionary to hold root data from XML file\n",
    "    element_data = {}\n",
    "    #iterate over each attribute of element\n",
    "    for attr in elem.attrib:\n",
    "        # add the attribute to dictionary\n",
    "        element_data[attr] = elem.attrib[attr]\n",
    "    # append element dictionary to the data list\n",
    "    json_data.append(element_data)\n",
    "\n",
    "#write/dump data to json file\n",
    "with open('xml_to_json.json', 'w') as xml_to_json:\n",
    "    json.dump(json_data, xml_to_json)\n",
    "\n",
    "#parse converted json file and read into new CSV for HR details\n",
    "with open('xml_to_json.json') as json_to_csv:\n",
    "    json_to_csv_data = json.load(json_to_csv)\n",
    "\n",
    "#open a file for writing and create a csv writer object\n",
    "HR_det = open('HR_det.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(HR_det)\n",
    "\n",
    "# Counter variable used for writing headers to the CSV file\n",
    "count = 0\n",
    "for item in json_to_csv_data:\n",
    "    if count == 0:\n",
    "        #Writing headers of CSV file\n",
    "        header = item.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    "##write rows\n",
    "for item in json_to_csv_data:\n",
    "    csv_writer.writerow(item.values())\n",
    "#close csv file\n",
    "HR_det.close() #close file\n",
    "\n",
    "#Merging with global variable\n",
    "with open(\"HR_det.csv\", mode='r') as HR_det2:\n",
    "    csv_reader = csv.reader(HR_det2, delimiter=',')\n",
    "    #user_data_csv = csv_reader\n",
    "    for line_number, line in enumerate(csv_reader):\n",
    "        if line_number == 0:\n",
    "            continue\n",
    "        first_name.append(line[0])\n",
    "        last_name.append(line[1])\n",
    "        age.append(line[2])\n",
    "        retired.append(line[4])\n",
    "        dependants.append(line[5])\n",
    "        marital_status.append(line[6])\n",
    "        salary.append(line[7])\n",
    "        pension.append(line[8])\n",
    "        company.append(line[9])\n",
    "        comm_dist.append(line[10])\n",
    "print(\"XML File Successfully Parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151f5fb",
   "metadata": {},
   "source": [
    "## Parsing files: TXT\n",
    "The below code is used to read the TXT file provided (user_data.txt). Information retreived from TXT file would be used to update the database as I proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9c0194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT File Successfully Parsed\n",
      "\"Shane Chambers e-mailed in overnight (Full details in Ticket #1839). During account creation something went wrong and their security code is wrong on their billing information. Bank is rejecting any payment until it's corrected. They're not sure what happened, but said to try \"935\". Can you please action this and try re-bill the client? Let me know if there's any further issues with it. Thanks\"\n",
      "\"Congratulations on the promotion Lane! We wouldn't have survived through the pandemic without you and your team. As a token of our appreciation, we've given you a Â£2100 salary bump. This will take effect as of next month's payroll. At Lewis-Johnson we value the care and work you put in. See you on Monday Joshua, enjoy!\"\n",
      "\"Happy Birthday Ms Suzanne Wright! You're 37 today. Our latest offers will be sure to get you into the party spirit!\"\n",
      "\"Hannah, the pension policy has changed slightly since the meeting last week. I know you've just finished putting through all the changes, but I need you to look at Mr Dunn's file. We'll need to  modify it by adding another 0.15% on top of the existing Â£22358. Drop me a message when you've done this please, it's quite urgent.\"\n"
     ]
    }
   ],
   "source": [
    "#opening the file in read mode\n",
    "txt_file = io.open(\"user_data.txt\", \"r\")\n",
    "\n",
    "#reading the file\n",
    "data = txt_file.read()\n",
    "print(\"TXT File Successfully Parsed\")\n",
    "print(data)\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578257a",
   "metadata": {},
   "source": [
    "## Creation of Unique Identifier\n",
    "The below code is used to create a unique value to identify each customer. This would serve as the primary key in the database. The code is a list comprehension method used in a loop to concatenate first name, last name and age (a combination unique to all customers).\n",
    "\n",
    "The for loop uses the length of the list: first_name as a stop criteria. \n",
    "\n",
    "The lists used for the list comprehension are global variables and has been updated with each file parsed (except the TXT file); therefore, each customer ID will appear 3 times.\n",
    "\n",
    "The duplicates are also taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1613327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID Count: 3000 Customers\n",
      "Unique Customer ID Count: 1000 Customers\n"
     ]
    }
   ],
   "source": [
    "#using list comprehension, concatenating to create a cus_id\n",
    "cus_id = [[first_name[i] + last_name[i] + age[i]] for i in range(len(first_name))] \n",
    "\n",
    "#getting the unique list of ID \n",
    "unique_id = set(x for l in cus_id for x in l)\n",
    "\n",
    "#creating a new set of list to write to csv\n",
    "veh_det_cus_id = cus_id[0:1000]\n",
    "fin_det_cus_id = cus_id[1000:2000]\n",
    "HR_det_cus_id = cus_id[2000:3000]\n",
    "list_header = [['cus_id']]\n",
    "veh_cus_id = list_header + veh_det_cus_id\n",
    "fin_cus_id = list_header + fin_det_cus_id\n",
    "HR_cus_id = list_header + HR_det_cus_id\n",
    "\n",
    "print(\"Customer ID Count:\",len(cus_id), \"Customers\")\n",
    "print(\"Unique Customer ID Count:\",len(unique_id), \"Customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfe512",
   "metadata": {},
   "source": [
    "## Assigning Customer ID to Data Sets\n",
    "\n",
    "The below code is used to match the customer IDs from the list created above to the files generated earlier that hold their respective information (user_data.csv, user_fin_det.csv, HR_det.csv).\n",
    "\n",
    "The matched record are then stored in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfdf8e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID Successfully Assigned\n"
     ]
    }
   ],
   "source": [
    "#matching cus_id to vehicle details (cus_id, firstname, lastname, age)\n",
    "with open('user_data.csv', 'r') as veh_det:\n",
    "      \n",
    "    # create a csv reader object for personal details\n",
    "    csv_reader2 = csv.reader(veh_det) \n",
    "    \n",
    "    # create a csv writer object \n",
    "    with open('veh_det_static_id.csv', 'w', newline='') as veh_det_static_id:\n",
    "        csv_writer = csv.writer(veh_det_static_id)\n",
    "        \n",
    "        # loop through each row in each file\n",
    "        for row1, row2 in zip(veh_cus_id, csv_reader2):\n",
    "            # create a new row by combining the current row from each file\n",
    "            new_row = row1 + row2\n",
    "            # write the new row to the new csv file\n",
    "            csv_writer.writerow(new_row)\n",
    "            \n",
    "# matching cus_id to financial details (cus_id, firstname, lastname, age)\n",
    "with open('user_fin_det.csv', 'r') as fin_det:\n",
    "      \n",
    "    # create a csv reader object for personal details\n",
    "    csv_reader2 = csv.reader(fin_det) \n",
    "    # create a csv writer object \n",
    "    with open('fin_det_static_id.csv', 'w', newline='') as fin_det_static_id:\n",
    "        csv_writer = csv.writer(fin_det_static_id)\n",
    "        \n",
    "        # loop through each row in each file\n",
    "        for row1, row2 in zip(fin_cus_id, csv_reader2):\n",
    "            # create a new row by combining the current row from each file\n",
    "            new_row = row1 + row2\n",
    "            # write the new row to the new csv file\n",
    "            csv_writer.writerow(new_row)\n",
    "fin_det_static_id.close()            \n",
    "    \n",
    "# matching unfiltered cus_id to personal details (cus_id, firstname, lastname, age)\n",
    "with open('HR_det.csv', 'r') as HR_det:\n",
    "    \n",
    "    # create a csv reader object for personal details\n",
    "    csv_reader2 = csv.reader(HR_det) \n",
    "    \n",
    "    # create a csv writer object \n",
    "    with open('HR_det_static_id.csv', 'w', newline='') as HR_det_static_id:\n",
    "        csv_writer = csv.writer(HR_det_static_id)\n",
    "        \n",
    "        # loop through each row in each file\n",
    "        for row1, row2 in zip(HR_cus_id, csv_reader2):\n",
    "            # create a new row by combining the current row from each file\n",
    "            new_row = row1 + row2\n",
    "            # write the new row to the new csv file\n",
    "            csv_writer.writerow(new_row)\n",
    "            \n",
    "print('Customer ID Successfully Assigned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0210e7",
   "metadata": {},
   "source": [
    "## Merging and Sorting of data\n",
    "The below code is used to sort the matched records created above in an ascending order using a lambda expression. The sorted matched record are then stored in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98942258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and Sorting Successful\n"
     ]
    }
   ],
   "source": [
    "#Open Files to be merged and sorted\n",
    "with open('HR_det_static_id.csv', 'r') as file1, open('veh_det_static_id.csv', 'r') as file2, open('fin_det_static_id.csv', 'r') as file3:\n",
    "    \n",
    "    #reader objects\n",
    "    csv_reader1 = csv.reader(file1) \n",
    "    csv_reader2 = csv.reader(file2) \n",
    "    csv_reader3 = csv.reader(file3) \n",
    "    #skip headers\n",
    "    next(csv_reader1)\n",
    "    next(csv_reader2)\n",
    "    next(csv_reader3)\n",
    "    #sorting using unique cus_ID\n",
    "    sortedlist1 = sorted(csv_reader1, key=lambda row: row[0])\n",
    "    sortedlist2 = sorted(csv_reader2, key=lambda row: row[0])\n",
    "    sortedlist3 = sorted(csv_reader3, key=lambda row: row[0])\n",
    "        \n",
    "    # create a csv writer object \n",
    "    with open('temp_file.csv', 'w', newline='') as temp_file:\n",
    "        #writer object\n",
    "        csv_writer = csv.writer(temp_file)\n",
    "        # loop through each row in each file\n",
    "        for row1, row2, row3 in zip(sortedlist1, sortedlist2, sortedlist3):\n",
    "            # create a new row by combining the current row from each file\n",
    "            new_row = row1 + row2 +row3\n",
    "            #skip first column\n",
    "            if new_row[0] !='':\n",
    "                # write the new row to the new csv file\n",
    "                csv_writer.writerow(new_row[0:34]) #taking out column that has debt\n",
    "print('Merging and Sorting Successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2d207",
   "metadata": {},
   "source": [
    "## Single Cohesive Record\n",
    "The below code deletes unwanted columns from temporary file (temp_file.csv) created above. The code also generates the consolidated record that would be moved to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff3aa047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated File Generated Successfully\n"
     ]
    }
   ],
   "source": [
    "#Create an empty list \n",
    "data = []\n",
    "\n",
    "# Open CSV file and create csv reader object \n",
    "with open('temp_file.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "\n",
    "    #Append data of csv reader object to the list \n",
    "    for row in csv_reader: \n",
    "        data.append(row)\n",
    "\n",
    "#Remove the column at index 2\n",
    "for row in data: \n",
    "    del row[12]\n",
    "    del row[12]\n",
    "    del row[12]\n",
    "    del row[12]\n",
    "    del row[12]    \n",
    "    del row[12]\n",
    "    del row[16]\n",
    "    del row[16]\n",
    "    del row[16]\n",
    "    del row[16]\n",
    "\n",
    "#Create a csv writer object \n",
    "with open('consolidated_data.csv', 'w', newline ='') as consolidated_data: \n",
    "    csv_writer = csv.writer(consolidated_data) \n",
    "    csv_writer.writerow(con_headers)\n",
    "    \n",
    "#writing rows\n",
    "    for row in data: \n",
    "        csv_writer.writerow(row)\n",
    "        \n",
    "print(\"Consolidated File Generated Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0594df0",
   "metadata": {},
   "source": [
    "## Loading into Database\n",
    "The below code establishes a connection with the database, creates a table (provides a criteria to drop the table if it already exist), sets the accepted value and length for each column.\n",
    "\n",
    "The consolidated file is also moved to the relational database at the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7f3b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Successfully Loaded!\n"
     ]
    }
   ],
   "source": [
    "db = Database()\n",
    "db.bind(provider=\"mysql\", host=host, user=user, passwd= password, db= database)\n",
    "\n",
    "#db.drop_table(Consolidated_data, if_exists=True, with_all_data=True) #overwrite table if already existing\n",
    "\n",
    "class Consolidated_data2(db.Entity):\n",
    "    cus_id = PrimaryKey(str, auto=False)\n",
    "    firstName = Required(str, max_len=30)\n",
    "    lastName = Required(str, max_len=30)\n",
    "    age = Required(int)\n",
    "    sex = Required(str, max_len=6)\n",
    "    retired = Required(str, max_len=6)\n",
    "    dependants = Optional(str)\n",
    "    marital_status = Required(str, max_len=30)\n",
    "    salary = Required(int)\n",
    "    pension = Required(float)\n",
    "    company = Required(str)\n",
    "    commute_distance = Required(float)\n",
    "    vehicle_make = Required(str, max_len=30)\n",
    "    vehicle_model = Required(str, max_len=45)\n",
    "    vehicle_year = Required(int)\n",
    "    vehicle_type = Required(str, max_len=45)\n",
    "    iban = Required(str, max_len=30)\n",
    "    credit_card_number = Required(str)\n",
    "    credit_card_security_code = Required(int)\n",
    "    credit_card_start_date = Required(str, max_len=6)\n",
    "    credit_card_end_date = Required(str, max_len=6)\n",
    "    address_main = Required(str, max_len=35)\n",
    "    address_city = Required(str, max_len=35)\n",
    "    address_postcode = Required(str, max_len=10)    \n",
    "db.generate_mapping(create_tables=True)\n",
    "\n",
    "with open('consolidated_data.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "        else:\n",
    "            with db_session:\n",
    "                #write the record from the CSV\n",
    "                Consolidated_data2(cus_id=row[0],\n",
    "                                        firstName=(row[1]),\n",
    "                                        lastName=(row[2]),\n",
    "                                        age=int(row[3]),\n",
    "                                        sex=(row[4]),\n",
    "                                        retired=(row[5]),\n",
    "                                        dependants=(row[6]),\n",
    "                                        marital_status=(row[7]),\n",
    "                                        salary=int(row[8]),\n",
    "                                        pension=int(row[9]),\n",
    "                                        company=(row[10]),\n",
    "                                        commute_distance=float(row[11]),\n",
    "                                        vehicle_make=(row[12]),\n",
    "                                        vehicle_model=(row[13]),\n",
    "                                        vehicle_year=int(row[14]),\n",
    "                                        vehicle_type=(row[15]),\n",
    "                                        iban=(row[16]),\n",
    "                                        credit_card_number=str(row[17]),\n",
    "                                        credit_card_security_code = int(row[18]),\n",
    "                                        credit_card_start_date = (row[19]),\n",
    "                                        credit_card_end_date = (row[20]),\n",
    "                                        address_main = (row[21]),\n",
    "                                        address_city = (row[22]),\n",
    "                                        address_postcode = (row[23]))\n",
    "                                        \n",
    "#Commit the changes\n",
    "db.commit()\n",
    "                                        \n",
    "print(\"Database Successfully Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cab71c",
   "metadata": {},
   "source": [
    "## Analysis and Update to the DataBase based on Memo (TXT File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c980525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving variable to info retreived from memo file to search through list of unique cus_id\n",
    "given_text = \"ShaneChambers\" #email sender name\n",
    "given_text2 = \"JoshuaLane\"  #recipient of promotion\n",
    "given_text3 = \"SuzanneWright\"  #celebrant\n",
    "given_info = '22358'  #existing pension amount for dunn, use amount to retrieve index from Global variable \"pension\"\n",
    "\n",
    "pen_bump_per = 0.15/100 #percentage increase\n",
    "pen_bump = float(pen_bump_per) * int(given_info)\n",
    "dunn_new_pen = round(pen_bump + int(given_info), 2)\n",
    "\n",
    "print(\"Using the information from the Memo to retrieve cus_id\\n\")\n",
    "for id_finder in unique_id: \n",
    "    if given_text in id_finder:\n",
    "        print(f\"{given_text} cus_id = {id_finder}\")\n",
    "    if given_text2 in id_finder:\n",
    "        print(f\"{given_text2} cus_id = {id_finder}\")\n",
    "    if given_text3 in id_finder:\n",
    "        print(f\"{given_text3} cus_id = {id_finder}\")      \n",
    "    else:\n",
    "        pass\n",
    "print('Dunn cus_id = %s' % HR_det_cus_id[pension.index(given_info)])\n",
    "print(\"\\nDunn's pension increase analysis\\n\\nPercentage increase according to Pension Policy:\" , pen_bump_per)\n",
    "print(\"Incremental Amount:\", pen_bump)\n",
    "print(\"New Pension Amount:\", dunn_new_pen)\n",
    "\n",
    "print(\"\\nJoshua's salary increase analysis\\n\")\n",
    "\n",
    "with db_session:\n",
    "    josh_sal = Consolidated_data.get(cus_id=\"JoshuaLane29\").salary\n",
    "    #print(\"Current Salary:\" , josh_sal)\n",
    "sal_after_prom = josh_sal + 2100\n",
    "\n",
    "if sal_after_prom <= 100049:\n",
    "    print(\"Current Salary:\" , josh_sal)\n",
    "    print(\"Salary Based on Promotion:\", sal_after_prom)\n",
    "else:\n",
    "    print(\"Joshua's Salary Information Already Updated\\nPlease see below:\\nCurrent Salary: 97949\\nSalary Based on Promotion: 100049\") \n",
    "\n",
    "    \n",
    "#Update an existing record \n",
    "with db_session: \n",
    "    query = Consolidated_data.select(lambda u: u.cus_id == \"ShaneChambers55\")\n",
    "    query2 = Consolidated_data.select(lambda u: u.cus_id == \"JoshuaLane29\")\n",
    "    query3 = Consolidated_data.select(lambda u: u.cus_id == \"SuzanneWright36\")\n",
    "    query4 = Consolidated_data.select(lambda u: u.cus_id == \"KyleDunn81\")\n",
    "    for update in query: \n",
    "        update.credit_card_security_code = \"935\" \n",
    "    for update in query2:\n",
    "        if sal_after_prom > 97949 and sal_after_prom <= 100049: #breaks the loop of constantly updating Joshua's salary\n",
    "            update.salary = sal_after_prom\n",
    "        else:\n",
    "            pass\n",
    "    for update in query3: \n",
    "        update.age = \"37\"\n",
    "    for update in query4: \n",
    "        update.pension = dunn_new_pen\n",
    "print(\"\\nResponse: All details Successfully Updated to Database based on Memo Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd275d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
